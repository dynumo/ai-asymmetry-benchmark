# AI Asymmetry Benchmark

This project benchmarks large language models (LLMs) for **asymmetry** â€” differences in how they respond to:

1. Questions about challenges faced by **marginalised or vulnerable groups**.
2. Questions critiquing **powerful actors and institutions**.

The goal is to measure if a model treats these domains differently in **accuracy**, **tone**, or **willingness to engage**.

## Status
ðŸš§ **Early development** â€” core scripts and dataset are being prepared.

## Planned Features
- Prompt dataset covering both domains.
- Scoring rubric for automated grading.
- Scripts for running prompts against any API model.
- Results stored with per-model and per-run folders for comparison.
- Optional analysis script for asymmetry scoring.

## Licence
MIT â€” see [LICENSE](LICENSE) for details.